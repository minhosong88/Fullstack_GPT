{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc GPT practice\n",
    "\n",
    " - RAG( Retrieval Augmented Generation) - models learn from data, some data are private and models cannot know. Prepare relevant documents to the prompt as a reference when models was not trained on that data. send the data to the model that was trained with other data. Increase the model's capability. => This is an example of stuff document. depending on other variables, you can choose what type of RAG you will use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrival\n",
    "    - Steps of Data connection: Source -> Load -> Transform -> Embed -> Store -> Retrieve\n",
    "\n",
    "    - Data loaders(: extract data from source and bring it to langchain)\n",
    "    - Splitters(Transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### unstructured file\n",
    " - support txt, pdf, ppt, html, images, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "# Loaders: TextLoader, PyPDFLoader ; There are many. Instead, you can use \n",
    "# UnstructuredFileLoader for various file types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "loader.load()\n",
    "# The return value of loader.load() is a list with a document in it.\n",
    "len(loader.load()) # output: 1\n",
    "# Better to find information from small chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - recursive character text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter()\n",
    "# Splitters try to keep the meaningful semantic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can split it into smaller chucks but this can destroy sentences.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # set chunk size: cause damage to the meaning\n",
    "    chunk_size=200,\n",
    "    # overlap the ending of the previous chunk to get meaningful structure.\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "len(docs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- character text splitter\n",
    "     - it has a separator property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = CharacterTextSplitter(\n",
    "    # put separator\n",
    "    separator=\"\\n\",\n",
    "    # set a max number of characters\n",
    "    chunk_size =600,\n",
    "    chunk_overlap=100,\n",
    "    # count length of the text by using len function by default\n",
    "    length_function=len,\n",
    "    # LLM does not count token by the length of text.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load_and_split(text_splitter=char_splitter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference between character and token\n",
    "- tokens are not characters.\n",
    "- models read text as id(numbers)\n",
    "<br>\n",
    "Let's implement token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align how the model counts with how we count\n",
    "token_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size =600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=token_splitter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding\n",
    "    - convert the human-reading text into computer-reading numbers\n",
    "- vectors: create vector for each document\n",
    "- openAI has more than 1,000 dimensions but here we focus on 3D vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example<br>\n",
    "3D: masculinity(m), femininity(f), royalty(r)<br>\n",
    "the word<br>\n",
    "'king' : 0.9m | 0.1f | 1.0r<br>\n",
    "'man'  : 0.9m | 0.1f | 0.0r<br>\n",
    "'queen': 0.1m | 0.9f | 1.0r<br>\n",
    "- you can do operation with words to get other words<br>\n",
    "king - man : <br>\n",
    "0.0m | 0.0f | 1.0r ==> 'royal'<br>\n",
    "'royal' : 0.0m | 0.0f | 1.0r<br>\n",
    "'woman': 0.1m | 0.9f | 0.0r<br>\n",
    "royal + woman ==> 0.1m | 0.9f| 1.0f : 'queen'<br>\n",
    "=> This is what embedding does<br><br>\n",
    "- take movies in the vectors. according to the vicinity in the vector, the algorithm can find you the movies similar to the movie you just watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "\n",
    "# Load variables from .env file\n",
    "env_vars = dotenv_values('.env')\n",
    "\n",
    "# Access the environment variables\n",
    "api_key = env_vars.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = OpenAIEmbeddings(openai_api_key= api_key)\n",
    "vector = embedder.embed_query(\"Hi\")\n",
    "len(vector) # 1536 dimensions for a word, 'hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_doc = embedder.embed_documents([\n",
    "    \"hi\",\n",
    "    \"How are you?\",\n",
    "    \"I am fine\",\n",
    "    \"and\",\n",
    "    \"you?\"\n",
    "])\n",
    "len(vector_doc) # 5\n",
    "len(vector_doc[0]) # 1536 each vector has the same length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When embedding, we are going to save the embed not to repeat embedding process.<br>\n",
    "<br>\n",
    "vector store saves vectors and allows searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use FAISS instead of Chroma, and don't forget to change all Chroma variables to FAISS\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"what did Eve eat?\")\n",
    "# This chunks will be sent to the model and the model will find answers from these documents: smaller pieces will help reduce costs\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cache embeddings\n",
    "    -   if we restart it, it will calculate the vectors again which costs money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve: search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"what did Eve eat?\")\n",
    "# This chunks will be sent to the model and the model will find answers from these documents: smaller pieces will help reduce costs\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Chain\n",
    "- here we are going to ask questions with our documents\n",
    "- different types of chain: stuff, refine, Map reduce, Map re-rank\n",
    "- off-the-shelf chain.\n",
    " - You can change the type of chain by modifying 'chain_type' argument."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stuff\n",
    "- insert all the document provided to the promtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, first we are using an off-the-shelf chain set up already.\n",
    "from langchain.chains import RetrievalQA\n",
    "llm = ChatOpenAI(api_key=api_key)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # default is 'stuff', here make it explicit on purpose\n",
    "    # This RetrievalQA chain require 'retriever' argument\n",
    "    # retriever: interface that \"return\" documents given queries.\n",
    "    # here we use vector store for it.\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"What was the context of Cain killing Abel?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refine\n",
    "- try to answer through each individual documents updating each time creating answers. The final answer should be the one the most refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"What happened to the lineage of Adam and Eve?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map reduce\n",
    "- summarize each individual documents, send over to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no mention of Adam speaking in the given portion of the document.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "chain.run(\"What did Adam say?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map re-rank\n",
    "- creat an answer from each documents and give scores to answers.\n",
    "- return the answer of the highest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_rerank\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "chain.run(\"What did Adam say?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL(langChain Expression Language) chain\n",
    "- Implement from prompt to output parser ourselves.\n",
    "- Customize the chain\n",
    "- Retriever takes a single-string argument(query) => return list of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='According to the context, after Eve and Adam ate the fruit from the tree of knowledge of good and evil, they realized they were naked and sewed fig leaves together to cover themselves. They then heard the voice of the LORD God walking in the garden and hid themselves. The LORD God called out to Adam and asked where he was. Eve is not specifically mentioned after this event.')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "# first get the documents with retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "# second get a prompt and send lists of documents(output of retriever) to the prompts\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using on;y the following context. If you don't know the answer just say you don't know, don't make it up:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "# third get llm chain to get the data from prompts\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    temperature=0.1,\n",
    "    )\n",
    "# create a chain in order\n",
    "\"\"\"chain = retriever | prompt | llm\"\"\"\n",
    "# The given question will be sent to the retriever, retrieve will return list of documents based on the question, documents will be the {context}, the given question will be {question}. \n",
    "#RunnablePassThrough: pass the input to the variable in the prompt\n",
    "chain = {\"context\": retriever, \"question\":RunnablePassthrough()} | prompt| llm \n",
    "# here use invoke() method, instead of run()\n",
    "chain.invoke(\"What happened to Eve?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce chain implementation\n",
    "- List of documents\n",
    "- for doc in list of documents | prompt | llm\n",
    "- iterating each document and send it through a prompt to llm and check if there is any relevant information\n",
    "- all responses from each llm will be put together into one document\n",
    "- the final document will be sent through  prompt to llm => final answer.\n",
    "- This approach is more appropriate due to the limited size that a prompt can contain: only relevant document will be sent to the final llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable Lambda class allows calling a function inside any chain.\n",
    "from langchain.schema.runnable import RunnableLambda     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Eve is the name of Adam's wife, who is referred to as the mother of all living. She is the woman that God created from one of Adam's ribs and brought to him in the garden of Eden.\")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "# set up prompt for individual document\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"\n",
    "     Use the following portion of a long docuement to see if any of the text is relevant to answer the question. Return any relevant text verbatim \n",
    "     -----\n",
    "     {context}\n",
    "     \n",
    "     \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "# set up a chain for individual document: return relevant information to the question\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# function to iterate each document with map_doc_chain\n",
    "def map_docs(inputs):\n",
    "    # The return has to be a single string\n",
    "    # define variables:note that the keys are the arguments from map_chain\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    # iterate documents with map_doc_chain\n",
    "    \"\"\"\n",
    "        results = []\n",
    "        for doc in documents:\n",
    "        result = map_doc_chain.invoke({\n",
    "            \"context\": doc.page_content,\n",
    "            \"question\": question\n",
    "            #document class has an attribute of content to bring the content of the document\n",
    "        }).content\n",
    "        # append the result into results list\n",
    "        results.append(result)\n",
    "    results = \"\\n\\n\".join(results) \"\"\"\n",
    "    # better code: list of the results will be joined to a single string\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": question\n",
    "        }).content for doc in documents\n",
    "    )\n",
    "\n",
    "# set up another chain for document iteration: Here the documents are processed through map_doc_prompt with the map_doc function\n",
    "\n",
    "# the arguments in the first chain will be sent to map_doc function through RunnableLambda class. => After all the iteration, a single string is returned.\n",
    "map_chain = {\"documents\":retriever, \"question\":RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "# final prompt is used after all the individual prompts were processed.\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"\n",
    "     Given the following extracted parts of a long document and a question, create a final answer.\n",
    "     If you don't know the answer, just say that you don't know. Don't try to makw up an answer.\n",
    "     -----\n",
    "     {context}\n",
    "     \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "# context argument for the final chain is the responses from each map_chain\n",
    "# through map_chain, the return value is a string, and this will be sent to the final prompt with question given.\n",
    "chain ={\"context\":map_chain, \"question\":RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"What is Eve?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
