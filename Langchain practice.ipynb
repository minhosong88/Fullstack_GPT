{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Load variables from .env file\n",
    "env_vars = dotenv_values('.env')\n",
    "\n",
    "# Access the environment variables\n",
    "api_key = env_vars.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key= api_key, temperature=0.9, streaming= True, callbacks=[StreamingStdOutCallbackHandler()] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Template Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medic_prompt = PromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a high-speed combat medic. You strictly follow TCCC guideline to treat patients in the battlefields. Be specific with treatment such as the dose of medication and timeline to treat patietn\"),\n",
    "    (\"human\", \"I want to treat patients with {injury} injury\"),\n",
    "])\n",
    "\n",
    "medic_chain = medic_prompt | chat \n",
    "infantry_prompt = PromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an infantry man. You are in a battlefield. You can choose very specific injuries given types of injury. Be creative and detailed when making injuries\"),\n",
    "    (\"human\", \"our platoon got a patient with {type} injury, possible {complication} within 5 min without treatment\"),\n",
    "])\n",
    "\n",
    "infantry_chain = infantry_prompt | chat\n",
    "\n",
    "final_chain = {\"injury\":infantry_chain} | medic_chain\n",
    "final_chain.invoke({\n",
    "    \"type\":\"gun shot wounds to the chest\",\n",
    "    \"complication\":\"tension pneumothorax\"\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chain(Think about why it is called Langchain)<br>\n",
    "Langchain internally calls previously formatted template, chat.predict and parser for you.<br>\n",
    "This '|' syntax will allow a lot of other things: <br>\n",
    "For example, there are chain_1 and chain_2, each consisting of a different template and an output parser.<br>\n",
    "all = chain_1 | chain_2 | new_outputparser() will combine all the chains.<br>\n",
    "chain = template | chat | CommaOutputParser()<br>\n",
    "Input of template is dictionary and the output is PromptValue | Chat model takes the output of template as an input and return Chatmessage<br>\n",
    "| output parser takes Chatmessage as an input and return output <br>\n",
    "By using chain with a invoke method, we can delete a number of lines of codes. <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FewShotPrompt Template<br>\n",
    "Few Shot means you are giving a few examples to the model so the model can give an answer in a way that I want to have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't use from_template() method, this is how we should do this.\n",
    "t = PromptTemplate(\n",
    "    template=\"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "# Prompt Template can be saved at a disk and load\n",
    "# t.format() will cause validation error because template wants a variable{country}\n",
    "# t.format() does not send any variable\n",
    "t.format(country=\"France\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step No.1\n",
    " - set an example, you can bring the example from a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "        {\n",
    "            \"question\": \"What do you know about France?\",\n",
    "            \"answer\": \"\"\"\n",
    "            Here is what I know:\n",
    "            Capital: Paris\n",
    "            Language: French\n",
    "            Food: Wine and Cheese\n",
    "            Currency: Euro\n",
    "            \"\"\",\n",
    "        },\n",
    "       {\n",
    "            \"question\": \"What do you know about Italy?\",\n",
    "            \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Rome\n",
    "            Language: Italian\n",
    "            Food: Pizza and Pasta\n",
    "            Currency: Euro\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What do you know about Greece?\",\n",
    "            \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Athens\n",
    "            Language: Greek\n",
    "            Food: Souvlaki and Feta Cheese\n",
    "            Currency: Euro\n",
    "            \"\"\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chat.predict(\"What do you know about France?\") gives a very long answer.\n",
    "You want it to provide an answer in the same manner as illustrated in the example\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FewShotPrompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#example_template=\"\"\"\n",
    "#    Human: {question}\n",
    "#    AI: {answer}\n",
    "#\"\"\" Notice that {question} and {answer} are the properties in your example template\n",
    "# You can create an example template like above or below\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples= examples,\n",
    "    #suffix is things coming at the end of the formatted examples, which is an input\n",
    "    suffix= \"Human: What do you know about {country}?\", # Here, the input is a question\n",
    "    #set variables used in the input \n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "# input_variables are set in the prompt, if the variables are not provided, there will be a validation error.\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Turkey\"\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"ai\",\"{answer}\")\n",
    "])\n",
    "# Message Prompts do not need suffix and input variables\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples= examples,\n",
    ")\n",
    "#Even though the prompt did not provide a formatted question with a \"country\" variable\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. You only give short answers\"),\n",
    "    example_prompt,\n",
    "    # this prompt only provides the name of the country and example with different country names\n",
    "    (\"human\", \"{country}\"),\n",
    "])\n",
    "chain = final_prompt | chat\n",
    "# The reply comes with the same format as illustrated in the example.\n",
    "chain.invoke({\n",
    "    \"country\":\"South Africa\"\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length Based Example Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Human:{question}\\nAI:{answer}\"\n",
    ")\n",
    "#This example selector limits the length of examples, determining how many examples the model chooses as examples.\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector= example_selector,\n",
    "    suffix= \"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Example Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector.base import BaseExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose \n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)    \n",
    "        \n",
    "        \n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        # As the type of input_variables is a list, select examples return a list with the selected example as string in the list\n",
    "        return [choice(self.examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Human:{question}\\nAI:{answer}\"\n",
    ")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector= example_selector,\n",
    "    suffix= \"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization and Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of prompt template:\n",
    "- .json\n",
    "- .yaml (easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"./prompt.json\")\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key= api_key, temperature=0.9, streaming= True, callbacks=[StreamingStdOutCallbackHandler()] )\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key= api_key, temperature=0.9, streaming= True, callbacks=[StreamingStdOutCallbackHandler()] )\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to gather multiple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 prompts. 3 are different prompts and the other is the last one that combines the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    "    )\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\":\"Pirates\",\n",
    "    \"example_question\":\"What is your location?\",\n",
    "    \"example_answer\":\"Arrrr! That is a secret!! Arg rg\",\n",
    "    \"question\":\"What is your favorite food?\",\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "Disable it when working on others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InMemoryCache saves the answer in memory\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "#set debug leaves a log what GPT is doing now.\n",
    "set_debug(True)\n",
    "#\n",
    "chat = ChatOpenAI(openai_api_key= api_key, temperature=0.9, \n",
    "                  #streaming= True, \n",
    "                  # callbacks=[StreamingStdOutCallbackHandler()]\n",
    "                  )\n",
    "chat.predict(\"How do you make an Italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I asked the same question again, I get the same answer immediately without further cost\n",
    "chat.predict(\"How do you make an Italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQLiteCache saves the data to a database\n",
    "set_llm_cache(SQLiteCache('cache.db'))\n",
    "# Go to langchain document-integration section to find more db options if you don't like SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When creating an answer, it creates a db file with the name provided in the SQLiteCache.\n",
    "chat.predict(\"How do you make an Italian pasta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization\n",
    "- How to know how much money spent on the model\n",
    "- How to save and load the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how much it costs to generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe of soju?\")\n",
    "    b = chat.predict(\"What is the recipe of bread?\")\n",
    "    print(usage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "chat2 = OpenAI(openai_api_key= api_key,\n",
    "               temperature=0.1,\n",
    "               max_tokens=450,\n",
    "               model=\"gpt-3.5-turbo-16k\")\n",
    "#if you want to save a model info, use save() method.\n",
    "#chat2.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "from langchain.llms.loading import load_llm\n",
    "# For some reason, this code does not work.\n",
    "chat3 = load_llm(\"model.json\")\n",
    "print(chat3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory\n",
    " - 5 types of memory: \n",
    " 1. conversation buffer memeory: save the whole conversation. Memory keeps growing.\n",
    " - without memory, chatbot cannot remember previous conversation, making it impossible for follow-up conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context(\n",
    " {\"input\":\"Hi!\"}, {\"output\": \"How are you?\"}\n",
    ")\n",
    "#This shows history of conversation. Repetition makes the memory grow and cost you more money\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMemory\n",
    "- Save a certain part of the conversation ex. a most recent few messages set by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    # k parameter refers to the number of messages\n",
    "    k=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "add_message(5,5)\n",
    "# k = 4, the first message is deleted due to the limit.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationSummaryMemory\n",
    " - Make use of llm.\n",
    " - Make a summary of the conversation\n",
    " - Use more token in the beginning, but less token as the conversation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi, I am Minho, I live in Olympia, WA\", \"Wow, that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"This place is really rainy\", \"Wow, that is also cool\")\n",
    "get_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationSummaryBufferMemory\n",
    "- mixed with summary and bufferwindow\n",
    "- keeps counting how many messages have exchanged until it reaches the limit\n",
    "- Then, summarize the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "    \n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Hi, I am Minho, I live in Olympia, WA\", \"Wow, that is so cool\")\n",
    "add_message(\"This place is really rainy\", \"Wow, that is also cool\")\n",
    "add_message(\"Where do you live?\", \"I live in Brazil\")\n",
    "add_message(\"Where do you live?\", \"I live in Brazil\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationKGMemory\n",
    "- build a knowledge graph in the entity on course of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=chat,\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"Hi, I am Minho, I live in Olympia, WA\", \"Wow, that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"input\":\"who is Minho\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Minho likes kimchi, jabchae, bulgogi\", \"Wow, that is so cool\")\n",
    "memory.load_memory_variables({\"input\":\"what does minho like?\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory on LLM chain\n",
    "- how to connect memory with chain\n",
    " 1. llm chain: an off-the-shelf chain(good way to start but you want to customize with langchain express language in the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    "    # memory_key is where the result of the memory goes to in the template\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "# make a space for the memory in the template\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "    \n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm= chat,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"My name is Minho\")\n",
    "chain.predict(question=\"I live in Olympia, WA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatbasedMemory\n",
    "- sending memory as strings or messages\n",
    "- How can we change it as a chatbased memory?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "# Change the prompt to chat prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # summary from summary memory added to the system message\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    # passing chat history to the template? =>message placeholder\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm= chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"My name is Minho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"I live in Olympia, WA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain expression language\n",
    " - customize the chain, manual but not hard\n",
    " - X using LLM chain, but memory code, prompt code x change either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"My name is Minho\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"My name is Minho\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel > 3:chain:load_memory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"My name is Minho\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel > 3:chain:load_memory] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"My name is Minho\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a helpful AI talking to a human\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"My name is Minho\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful AI talking to a human\\nHuman: My name is Minho\"\n",
      "  ]\n",
      "}\n",
      "Hello Minho! How can I assist you today?\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:llm:ChatOpenAI] [1.39s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hello Minho! How can I assist you today?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"Hello Minho! How can I assist you today?\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.40s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "content='Hello Minho! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # summary from summary memory added to the system message\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    # passing chat history to the template? =>message placeholder\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "def load_memory(input):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "# call load_memory function, which goesto chat_history\n",
    "chain = RunnablePassthrough.assign(chat_history= load_memory) | prompt | chat\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    # invoke the chain\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    # save the interaction in the memory\n",
    "    memory.save_context({\"input\": question}, {\"output\":result.content})\n",
    "    print(result) \n",
    "    \n",
    "invoke_chain(\"My name is Minho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Whay is my name?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Whay is my name?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel > 3:chain:load_memory] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Whay is my name?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel > 3:chain:load_memory] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"My name is Minho\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"AIMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"Hello Minho! How can I assist you today?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a helpful AI talking to a human\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"My name is Minho\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Hello Minho! How can I assist you today?\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Whay is my name?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful AI talking to a human\\nHuman: My name is Minho\\nAI: Hello Minho! How can I assist you today?\\nHuman: Whay is my name?\"\n",
      "  ]\n",
      "}\n",
      "Your name is Minho.\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:llm:ChatOpenAI] [1.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Your name is Minho.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"Your name is Minho.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.71s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "content='Your name is Minho.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Whay is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
